# Sistema RAG Avanzado con LangChain

**Autora:** Alison Geraldine Valderrama Munar  
**Curso:** AREP - Arquitecturas Empresariales  
**Universidad:** Escuela Colombiana de IngenierÃ­a Julio Garavito

## DescripciÃ³n General

Este proyecto implementa un sistema de **Retrieval-Augmented Generation (RAG)** de Ãºltima generaciÃ³n que permite realizar consultas inteligentes y contextuales sobre documentaciÃ³n web especializada. El sistema combina la potencia de los Large Language Models (LLMs) de OpenAI con bÃºsqueda semÃ¡ntica avanzada en bases de datos vectoriales para proporcionar respuestas precisas, fundamentadas y contextualizadas en tiempo real.

### Â¿QuÃ© es RAG?

RAG (Retrieval-Augmented Generation) es una arquitectura avanzada de IA que potencia las capacidades de los modelos de lenguaje al integrar un sistema de recuperaciÃ³n de informaciÃ³n externa antes de generar respuestas. Esta tÃ©cnica hÃ­brida:

- **Reduce significativamente las alucinaciones** del modelo mediante anclaje en datos verificables
- **Permite trabajar con informaciÃ³n actualizada** sin necesidad de reentrenamiento costoso del modelo
- **Mejora la precisiÃ³n y relevancia** de las respuestas mediante bÃºsqueda semÃ¡ntica contextual
- **Facilita la trazabilidad** al proporcionar las fuentes de informaciÃ³n utilizadas

## Stack TecnolÃ³gico

- **[LangChain](https://www.langchain.com/)**: Framework orquestador para desarrollo de aplicaciones avanzadas con LLMs
- **[OpenAI GPT-4o](https://openai.com/)**: Modelo de lenguaje de Ãºltima generaciÃ³n para comprensiÃ³n y generaciÃ³n de texto
- **[OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings)**: text-embedding-3-small (512 dimensiones) para representaciÃ³n vectorial semÃ¡ntica
- **[Pinecone](https://www.pinecone.io/)**: Base de datos vectorial serverless de alto rendimiento para bÃºsqueda semÃ¡ntica escalable
- **[BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/)**: LibrerÃ­a robusta para parsing y extracciÃ³n de contenido HTML
- **[python-dotenv](https://pypi.org/project/python-dotenv/)**: GestiÃ³n segura de variables de entorno y credenciales
- **Python 3.10+**: Lenguaje de programaciÃ³n principal con soporte completo para async/await

## Arquitectura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Fuente de Datos    â”‚
â”‚   (Documentos Web)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   WebBaseLoader      â”‚ â”€â”€â–º ExtracciÃ³n selectiva de contenido HTML
â”‚   + BeautifulSoup    â”‚     (tÃ­tulo, headers, contenido principal)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RecursiveCharacter   â”‚ â”€â”€â–º FragmentaciÃ³n inteligente
â”‚    TextSplitter      â”‚     â€¢ Chunks: 1000 caracteres
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â€¢ Overlap: 200 caracteres
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OpenAI Embeddings   â”‚ â”€â”€â–º VectorizaciÃ³n semÃ¡ntica
â”‚ text-embedding-3     â”‚     â€¢ DimensiÃ³n: 512
â”‚      -small          â”‚     â€¢ Modelo: small
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Pinecone Index     â”‚ â”€â”€â–º Almacenamiento vectorial
â”‚   (arep-taller)      â”‚     â€¢ MÃ©trica: cosine
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â€¢ Tipo: serverless
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    RAG Agent         â”‚ â”€â”€â–º Motor de recuperaciÃ³n y generaciÃ³n
â”‚   GPT-4o + Tools     â”‚     1. BÃºsqueda semÃ¡ntica (top-k=2)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     2. ConstrucciÃ³n de contexto
           â”‚                 3. GeneraciÃ³n de respuesta
           â–¼                 4. Streaming en tiempo real
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Respuesta Streaming  â”‚
â”‚   Contextualizada    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## InstalaciÃ³n y ConfiguraciÃ³n

### 1. Clonar el repositorio

```bash
git clone https://github.com/LIZVALMU/Taller_RAGs.git
cd Taller_RAGs
```

### 2. Crear y activar entorno virtual

```bash
# Crear entorno virtual
python -m venv .venv

# Activar entorno virtual
# En Windows PowerShell:
.\.venv\Scripts\activate

# En Linux/Mac:
source .venv/bin/activate
```

### 3. Instalar dependencias

```bash
# Actualizar pip
python -m pip install --upgrade pip

# Instalar dependencias del proyecto
pip install -q langchain langchain-text-splitters langchain-community bs4
pip install -qU langchain-openai
pip install -qU langchain-pinecone
pip install -q python-dotenv

# Verificar instalaciÃ³n
pip list | grep -E "langchain|openai|pinecone"
```

**Alternativa:** Instalar desde archivo de requisitos
```bash
pip install -r requirements.txt
```

### 4. Configurar variables de entorno

Crea un archivo `.env` en la raÃ­z del proyecto con tus credenciales:

```env
# ================================
# OpenAI Configuration
# ================================
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxxxxxxxxxx

# ================================
# Pinecone Configuration
# ================================
PINECONE_API_KEY=pcsk_xxxxxxxxxxxxxxxxxxxxx
PINECONE_INDEX_NAME=arep-taller

# ================================
# LangSmith Configuration (Opcional)
# Para debugging y tracing de agentes
# ================================
LANGCHAIN_API_KEY=lsv2_pt_xxxxxxxxxxxxxxxxxxxxx
LANGCHAIN_TRACING_V2=true
LANGCHAIN_PROJECT=rag-system-alison-valderrama
```

**Seguridad:** AsegÃºrate de agregar `.env` a tu `.gitignore` para no exponer tus credenciales.

#### Obtener las API Keys:

| Servicio | URL | Notas |
|----------|-----|-------|
| **OpenAI** | https://platform.openai.com/api-keys | Requiere cuenta con crÃ©ditos activos |
| **Pinecone** | https://app.pinecone.io/ | Plan gratuito disponible (1 Ã­ndice serverless) |
| **LangChain** | https://smith.langchain.com/ | Opcional - Para debugging y monitoreo |

### 5. Configurar Ã­ndice en Pinecone

**Pasos para crear el Ã­ndice:**

1. Accede a [Pinecone Console](https://app.pinecone.io/)
2. Crea un nuevo Ã­ndice con la siguiente configuraciÃ³n:

| ParÃ¡metro | Valor | DescripciÃ³n |
|-----------|-------|-------------|
| **Nombre** | `arep` | Debe coincidir con `PINECONE_INDEX_NAME` en `.env` |
| **DimensiÃ³n** | `1024` | Compatible con text-embedding-3-large |
| **MÃ©trica** | `cosine` | MÃ©trica de similitud coseno para embeddings |
| **Tipo** | `Serverless` | Sin infraestructura que gestionar |
| **RegiÃ³n** | `AWS us-east-1` | Mejor latencia para la mayorÃ­a de usuarios |

3. Espera a que el Ã­ndice estÃ© en estado `Ready`
4. Copia la API Key desde el dashboard de Pinecone

## Uso del Sistema

### Ejecutar el Notebook

**OpciÃ³n 1: VS Code (Recomendado)**
1. Abre VS Code en el directorio del proyecto
2. Instala la extensiÃ³n "Jupyter" de Microsoft
3. Abre el archivo `Taller_Rag_Agent_Lang.ipynb`
4. Selecciona el kernel de Python (`.venv`)
5. Ejecuta las celdas secuencialmente con `Shift + Enter`

**OpciÃ³n 2: Jupyter Notebook**
```bash
# Instalar Jupyter si no lo tienes
pip install jupyter

# Iniciar Jupyter Notebook
jupyter notebook

# Abre Taller_Rag_Agent_Lang.ipynb desde la interfaz web
```

**OpciÃ³n 3: JupyterLab**
```bash
# Instalar JupyterLab
pip install jupyterlab

# Iniciar JupyterLab
jupyter lab
```

### Estructura del Notebook

El notebook estÃ¡ organizado en 5 mÃ³dulos principales:

#### Setup - ConfiguraciÃ³n del Entorno
- **InstalaciÃ³n de dependencias** necesarias para el sistema RAG
- **Carga de variables de entorno** desde archivo `.env`
- **InicializaciÃ³n de modelos de IA**:
  - GPT-4o para generaciÃ³n de respuestas
  - text-embedding-3-small para vectorizaciÃ³n (512 dims)
- **ConfiguraciÃ³n de Pinecone** y creaciÃ³n del vector store

#### 1. Pipeline de IndexaciÃ³n
- **1.1 ExtracciÃ³n de Contenido Web**: 
  - WebBaseLoader extrae contenido del blog de Lilian Weng
  - BeautifulSoup filtra elementos relevantes (tÃ­tulo, headers, contenido)
- **1.2 InspecciÃ³n del Contenido**: 
  - Vista previa del documento extraÃ­do
- **1.3 FragmentaciÃ³n Inteligente**: 
  - RecursiveCharacterTextSplitter divide el documento
  - Chunks de 1000 caracteres con overlap de 200
- **1.4 IndexaciÃ³n Vectorial**: 
  - ConversiÃ³n a embeddings de 512 dimensiones
  - Almacenamiento en Pinecone con metadatos

#### 2. Sistema de RecuperaciÃ³n y GeneraciÃ³n
- **2.1 Herramienta de BÃºsqueda SemÃ¡ntica**: 
  - FunciÃ³n `fetch_relevant_context` para recuperaciÃ³n
  - Top-k=2 documentos mÃ¡s relevantes por consulta
- **2.2 ConfiguraciÃ³n del Agente RAG**: 
  - IntegraciÃ³n de GPT-4o con herramientas de bÃºsqueda
  - System prompt para uso efectivo del contexto

#### 3-4. DemostraciÃ³n y ValidaciÃ³n
- **Consulta compleja** con mÃºltiples bÃºsquedas iterativas
- **Consulta simple** para verificaciÃ³n bÃ¡sica
- **Streaming en tiempo real** de respuestas

#### 5. AuditorÃ­a del Vector Store
- EstadÃ­sticas detalladas del Ã­ndice Pinecone
- Prueba de bÃºsqueda semÃ¡ntica
- VerificaciÃ³n de salud del sistema

## Ejemplo de Uso

```python
# Realizar una consulta al sistema RAG
user_question = "What is task decomposition?"

print(f"Consulta: {user_question}\n")

for stream_event in rag_agent.stream(
    {"messages": [{"role": "user", "content": user_question}]},
    stream_mode="values",
):
    stream_event["messages"][-1].pretty_print()
```

### Flujo de Procesamiento

```
Usuario hace pregunta
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   RAG Agent (GPT-4o)   â”‚
â”‚  Analiza la consulta   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ fetch_relevant_context â”‚
â”‚ BÃºsqueda semÃ¡ntica en  â”‚
â”‚   Pinecone (top-k=2)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Documentos relevantes  â”‚
â”‚   con metadatos y      â”‚
â”‚   similarity scores    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GPT-4o genera        â”‚
â”‚  respuesta basada en   â”‚
â”‚  contexto recuperado   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Respuesta streaming    â”‚
â”‚ en tiempo real al user â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ConfiguraciÃ³n Avanzada

### Ajustar TamaÃ±o de Fragmentos

Modifica el tamaÃ±o de los chunks segÃºn la naturaleza de tus documentos:

```python
document_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500,      # Aumentar para documentos tÃ©cnicos densos
    chunk_overlap=300,    # Mayor superposiciÃ³n para mejor contexto
    add_start_index=True,
)
```

**Recomendaciones:**
- **Documentos tÃ©cnicos**: chunk_size=1500-2000, overlap=300-400
- **ArtÃ­culos cortos**: chunk_size=800-1000, overlap=150-200
- **DocumentaciÃ³n legal**: chunk_size=2000-3000, overlap=400-500

### Optimizar RecuperaciÃ³n de Documentos

Ajusta el nÃºmero de documentos recuperados (k) segÃºn precisiÃ³n vs contexto:

```python
@tool(response_format="content_and_artifact")
def fetch_relevant_context(user_query: str):
    """Recupera informaciÃ³n contextual relevante."""
    relevant_docs = rag_vector_store.similarity_search(
        user_query, 
        k=5  # Aumentar para mÃ¡s contexto (pero mÃ¡s tokens)
    )
    # ... resto del cÃ³digo
```

**Trade-offs:**
- **k=2**: Respuestas mÃ¡s precisas, menos contexto
- **k=5**: MÃ¡s contexto, mayor costo en tokens
- **k=10**: Contexto exhaustivo, riesgo de informaciÃ³n irrelevante

### Cambiar Modelo de Lenguaje

Experimenta con diferentes modelos segÃºn tus necesidades:

```python
# Opciones disponibles
llm_model = init_chat_model("gpt-4o", model_provider="openai")        # Mejor calidad
llm_model = init_chat_model("gpt-4-turbo", model_provider="openai")   # Balance
llm_model = init_chat_model("gpt-3.5-turbo", model_provider="openai") # MÃ¡s econÃ³mico
```

**ComparaciÃ³n de modelos:**

| Modelo | Costo | Calidad | Velocidad | Uso Recomendado |
|--------|-------|---------|-----------|-----------------|
| GPT-4o | Alto | Excelente | RÃ¡pido | ProducciÃ³n, anÃ¡lisis complejo |
| GPT-4-turbo | Medio-Alto | Muy buena | Medio | Balance general |
| GPT-3.5-turbo | Bajo | Buena | Muy rÃ¡pido | Desarrollo, pruebas |

### Configurar Embeddings

Ajusta las dimensiones de los embeddings segÃºn tus necesidades:

```python
# 512 dimensiones (recomendado para este proyecto)
embedding_model = OpenAIEmbeddings(
    model="text-embedding-3-small", 
    dimensions=512
)

# 1536 dimensiones (mayor precisiÃ³n, mÃ¡s costoso)
embedding_model = OpenAIEmbeddings(
    model="text-embedding-3-large", 
    dimensions=1536
)
```

**Importante:** Si cambias las dimensiones, debes recrear el Ã­ndice de Pinecone con la nueva configuraciÃ³n.

## CaracterÃ­sticas Principales del Sistema

### Capacidades Funcionales

-  **BÃºsqueda SemÃ¡ntica Avanzada**: Encuentra informaciÃ³n basÃ¡ndose en significado contextual, no solo coincidencia de palabras clave
-  **Streaming en Tiempo Real**: Respuestas generadas y mostradas progresivamente para mejor UX
-  **Contexto Conversacional**: Mantiene coherencia entre mÃºltiples interacciones consecutivas
-  **Trazabilidad de Fuentes**: Cada respuesta incluye referencias a los documentos fuente utilizados
-  **BÃºsquedas Iterativas**: El agente puede realizar mÃºltiples bÃºsquedas para consultas complejas
### CaracterÃ­sticas TÃ©cnicas

-  **Escalabilidad Horizontal**: Pinecone soporta millones de vectores sin degradaciÃ³n de rendimiento
-  **Arquitectura Modular**: FÃ¡cil extensiÃ³n con nuevas herramientas, fuentes de datos y modelos
-  **GestiÃ³n de Embeddings**: Sistema eficiente de vectorizaciÃ³n con 512 dimensiones
-  **OptimizaciÃ³n de Costos**: ConfiguraciÃ³n ajustable para balancear calidad vs costo
-  **Serverless**: Sin infraestructura que gestionar, escalado automÃ¡tico

### Seguridad y Confiabilidad

-  **GestiÃ³n Segura de Credenciales**: Variables de entorno con python-dotenv
-  **ValidaciÃ³n de Inputs**: VerificaciÃ³n de API keys y configuraciones requeridas
-  **Manejo de Errores**: Sistema robusto de logging y error handling
-  **ReducciÃ³n de Alucinaciones**: Respuestas ancladas en documentos verificables

## Notas Importantes y Consideraciones

### Sobre Pinecone

| ParÃ¡metro | Valor Requerido | Notas |
|-----------|----------------|-------|
| **DimensiÃ³n** | `1024` | Debe coincidir con text-embedding-3-large |
| **MÃ©trica** | `cosine` | Mejor para embeddings de texto |
| **RegiÃ³n** | `us-east-1` (AWS) | Latencia Ã³ptima para AmÃ©rica |
| **Tipo** | `Serverless` | Sin gestiÃ³n de infraestructura |

**Importante**: Si cambias el modelo de embeddings, debes recrear el Ã­ndice con las nuevas dimensiones.

### Estructura de Costos

#### OpenAI (Precios aproximados - Verificar precios actuales)

| Servicio | Entrada | Salida | Notas |
|----------|---------|--------|-------|
| **GPT-4o** | ~$5.00 / 1M tokens | ~$15.00 / 1M tokens | Ãšltima generaciÃ³n |
| **GPT-4-turbo** | ~$10.00 / 1M tokens | ~$30.00 / 1M tokens | Balance calidad/precio |
| **GPT-3.5-turbo** | ~$0.50 / 1M tokens | ~$1.50 / 1M tokens | OpciÃ³n econÃ³mica |
| **text-embedding-3-small** | ~$0.02 / 1M tokens | - | VectorizaciÃ³n eficiente |
| **text-embedding-3-large** | ~$0.13 / 1M tokens | - | Mayor precisiÃ³n |

**EstimaciÃ³n de costos para este proyecto:**
- **IndexaciÃ³n inicial** (~42K tokens): ~$0.001
- **Consulta promedio** (~2K tokens): ~$0.01 - $0.03
- **100 consultas/dÃ­a**: ~$1 - $3/dÃ­a

#### Pinecone

- **Plan Gratuito**: 1 Ã­ndice serverless, suficiente para desarrollo y pruebas
- **Plan Starter**: A partir de $70/mes para producciÃ³n
- **Consultas**: Incluidas en el plan mensual

### LÃ­mites TÃ©cnicos y Consideraciones

| Componente | LÃ­mite | RecomendaciÃ³n |
|------------|--------|---------------|
| **text-embedding-3-small** | 8,191 tokens/input | Chunks < 1,500 caracteres |
| **GPT-4o** | 128K tokens contexto | Ã“ptimo con k=2-5 documentos |
| **Pinecone Free Tier** | 100K vectores | Suficiente para ~200 artÃ­culos |
| **Rate Limits OpenAI** | VarÃ­a por tier | Implementar retry logic |

### Consideraciones de ProducciÃ³n

1. **CachÃ© de Embeddings**: Implementar cachÃ© para vectores ya generados
2. **Batch Processing**: Procesar mÃºltiples documentos en paralelo
3. **Monitoring**: Usar LangSmith para tracking y debugging
4. **Error Handling**: Implementar reintentos con backoff exponencial
5. **Rate Limiting**: Respetar lÃ­mites de las APIs

## SoluciÃ³n de Problemas Comunes

### Error: "Invalid API Key" / AutenticaciÃ³n Fallida

**SÃ­ntomas:**
```
openai.error.AuthenticationError: Invalid API key provided
```

**Soluciones:**
1. Verifica que la API key de OpenAI sea vÃ¡lida y estÃ© activa
2. Confirma que tu cuenta tenga crÃ©ditos disponibles en [OpenAI Billing](https://platform.openai.com/account/billing)
3. Verifica que no haya espacios adicionales en el archivo `.env`
4. Recarga las variables de entorno:
   ```python
   from dotenv import load_dotenv
   load_dotenv(override=True)
   ```
5. Reinicia el kernel del notebook

### Error: "Index not found" / Error de Pinecone

**SÃ­ntomas:**
```
pinecone.exceptions.NotFoundException: Index 'arep-taller' not found
```

**Soluciones:**
1. Verifica que el nombre del Ã­ndice en `.env` coincida exactamente con el de Pinecone
2. Confirma que el Ã­ndice estÃ© en estado "Ready" en [Pinecone Console](https://app.pinecone.io/)
3. Verifica que estÃ©s usando la API key correcta del proyecto
4. Espera unos minutos si acabas de crear el Ã­ndice

### Error: "Dimension mismatch" / Dimensiones Incompatibles

**SÃ­ntomas:**
```
ValueError: Dimension mismatch: index has 1536 dimensions, but embeddings have 512
```

**Soluciones:**
1. El Ã­ndice debe tener exactamente **512 dimensiones** para text-embedding-3-small
2. Si cambiaste el modelo de embeddings, debes recrear el Ã­ndice:
   ```python
   # Eliminar Ã­ndice antiguo (âš ï¸ Cuidado: borra todos los datos)
   pc.delete_index(index_name)
   
   # Crear nuevo Ã­ndice con dimensiones correctas
   pc.create_index(
       name=index_name,
       dimension=512,
       metric='cosine',
       spec=ServerlessSpec(cloud='aws', region='us-east-1')
   )
   ```

### ğŸ”Œ Error: "Connection timeout" / Problemas de Red

**SÃ­ntomas:**
```
requests.exceptions.ConnectionError: Connection timeout
```

**Soluciones:**
1. Verifica tu conexiÃ³n a internet
2. Comprueba que no haya firewall bloqueando las APIs
3. Implementa reintentos con backoff:
   ```python
   from tenacity import retry, stop_after_attempt, wait_exponential
   
   @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
   def make_api_call():
       # Tu cÃ³digo aquÃ­
       pass
   ```

### Error: "Rate limit exceeded" / LÃ­mite de Tasa Excedido

**SÃ­ntomas:**
```
openai.error.RateLimitError: Rate limit exceeded
```

**Soluciones:**
1. Reduce la frecuencia de llamadas a la API
2. Implementa un sistema de cola con delays
3. Considera actualizar tu tier en OpenAI para mayores lÃ­mites
4. Usa batch processing para procesar mÃºltiples documentos eficientemente

### Otros Problemas Comunes

| Problema | SoluciÃ³n |
|----------|----------|
| **MÃ³dulo no encontrado** | `pip install -r requirements.txt` |
| **Kernel crash** | Reinicia el kernel y ejecuta celdas secuencialmente |
| **Respuestas vacÃ­as** | Verifica que el Ã­ndice tenga documentos: `index.describe_index_stats()` |
| **Costos altos** | Reduce `k` en similarity_search o usa GPT-3.5-turbo |

## Recursos y Referencias

### DocumentaciÃ³n Oficial

| Recurso | URL | DescripciÃ³n |
|---------|-----|-------------|
| **LangChain Docs** | https://python.langchain.com/ | Framework principal del proyecto |
| **OpenAI Platform** | https://platform.openai.com/docs/ | API reference completa |
| **Pinecone Docs** | https://docs.pinecone.io/ | Base de datos vectorial |
| **LangSmith** | https://docs.smith.langchain.com/ | Debugging y monitoring |

### Tutoriales y GuÃ­as

- [LangChain RAG Tutorial](https://python.langchain.com/docs/tutorials/rag/) - Tutorial oficial de RAG
- [Pinecone RAG Guide](https://www.pinecone.io/learn/retrieval-augmented-generation/) - GuÃ­a completa de RAG
- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings) - GuÃ­a de embeddings
- [LangChain Agents](https://python.langchain.com/docs/modules/agents/) - DocumentaciÃ³n de agentes

### ArtÃ­culos Relevantes

- [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) - Blog de Lilian Weng (fuente de datos del proyecto)
- [RAG vs Fine-tuning](https://www.pinecone.io/learn/rag-vs-finetuning/) - ComparaciÃ³n de tÃ©cnicas
- [Vector Database Comparison](https://www.pinecone.io/learn/vector-database/) - ComparaciÃ³n de bases de datos vectoriales

### Videos Educativos

- [LangChain Crash Course](https://www.youtube.com/watch?v=LbT1yp6quS8) - Intro a LangChain
- [RAG Explained](https://www.youtube.com/watch?v=T-D1OfcDW1M) - ExplicaciÃ³n de RAG
- [Pinecone Tutorial](https://www.youtube.com/watch?v=gTCU9I6QqCE) - Tutorial de Pinecone

### Repositorios Relacionados

- [LangChain Examples](https://github.com/langchain-ai/langchain/tree/master/docs/docs/tutorials) - Ejemplos oficiales
- [RAG Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Gen_QA.ipynb) - OpenAI cookbook
- [Awesome RAG](https://github.com/awesome-rag/awesome-rag) - Lista curada de recursos RAG

## Autora

**Alison Geraldine Valderrama Munar**

**Contacto:** AREP - Arquitecturas Empresariales  
**InstituciÃ³n:** Escuela Colombiana de IngenierÃ­a Julio Garavito  
**Repositorio:** https://github.com/LIZVALMU/Taller_RAGs

---

## Licencia
Este proyecto fue desarrollado como parte del curso de Arquitecturas Empresariales de la Escuela Colombiana de IngenierÃ­a Julio Garavito.

---

<div align="center">

**Si este proyecto te fue Ãºtil, no olvides darle una estrella en GitHub**

Desarrollado por Alison Valderrama

</div>  